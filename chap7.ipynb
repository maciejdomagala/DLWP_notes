{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# two versions, one with the training dataset, other with validation\n",
    "cifar_path = r\"C:\\Users\\DomagalaMa\\Desktop\\DLWP_notes\\DLWP_notes\\data\\CIFAR10\"\n",
    "cifar10 = datasets.CIFAR10(cifar_path, train=True, download=True)\n",
    "cifar10_val = datasets.CIFAR10(cifar_path, train=False, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cifar10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PIL.Image.Image image mode=RGB size=32x32 at 0x1F8277B8080>\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image, *_ = cifar10[99] #interested only in first value, the PIL image object\n",
    "print(image)\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 32, 32])\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "to_tensor = transforms.ToTensor()\n",
    "img_t = to_tensor(image)\n",
    "print(img_t.shape) #notice it's already in pytorch way - CxHxW\n",
    "\n",
    "#now load whole dataset with already transform to tensor\n",
    "\n",
    "tensor_cifar10 = datasets.CIFAR10(cifar_path, download=True, train=True, transform = transforms.ToTensor())\n",
    "\n",
    "img_t, *_ = tensor_cifar10[99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 32, 32]), torch.float32)"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_t.shape, img_t.dtype  #already in float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalizing the data part\n",
    "#we will calculate the parameters for normalization step\n",
    "\n",
    "\n",
    "#first, all of the data (50000 tensors) along new dimension\n",
    "imgs = torch.stack([img for img, *_ in tensor_cifar10], dim=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 51200000])"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs.view(3, -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4915, 0.4823, 0.4468])\n"
     ]
    }
   ],
   "source": [
    "mean_c = imgs.view(3, -1).mean(dim=1)\n",
    "print(mean_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2470, 0.2435, 0.2616])\n"
     ]
    }
   ],
   "source": [
    "#mean calculated, now standard deviation\n",
    "std_c = imgs.view(3, -1).std(dim=1)\n",
    "print(std_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading cifar once more, now with normalization\n",
    "\n",
    "#values from previous calculations\n",
    "mean = (0.4915, 0.4823, 0.4468)\n",
    "std = (0.2470, 0.2435, 0.2616)\n",
    "\n",
    "#loading with transform\n",
    "new_transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean, std)])\n",
    "transformed_cifar10 = datasets.CIFAR10(cifar_path, \n",
    "                                       download=True, \n",
    "                                       train=True, \n",
    "                                       transform = new_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_new, *_ = transformed_cifar10[99]\n",
    "plt.imshow(img_new.permute(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a model on only birds and airplanes\n",
    "#create a cifar10 subset called cifar2 with only these two types of labels included\n",
    "#notice that airplane - label 0, bird - label 2.\n",
    "#we will transfer bird to be label 1 in the newly created dataset\n",
    "\n",
    "label_mapping = {0:0, 2:1}\n",
    "\n",
    "cifar2 = [(image, label_mapping[label]) for (image, label) in transformed_cifar10 if (label in [0,2])]\n",
    "cifar2_val = [(image, label_mapping[label]) for (image, label) in transformed_cifar10 if (label in [0,2])]\n",
    "len(cifar2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a model to deal with the images\n",
    "\n",
    "#we will feed the data as one vector calculated by multiplying values from each dimension of the sample\n",
    "img, *_ = cifar2[16]\n",
    "plt.imshow(img.permute(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the input to the network will be samples of 3x32x32 which means 3072 input features\n",
    "\n",
    "import torch.nn as nn\n",
    "model = nn.Sequential(nn.Linear(3072, 512),\n",
    "                      nn.Tanh(),\n",
    "                      nn.Linear(512, 2),\n",
    "                      nn.Softmax(dim=1))  #the dim=1 means we want to apply the softmax to the first dimension, remembering\n",
    "                                          #that 0th dimension is bacth size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing for the single-element batch \n",
    "img_batch = img.view(-1).unsqueeze(0)  #modyfing the tensor shape so it matches with out input\n",
    "                                        #(we want batch size in 0th dimension and 3072 in 1st dim)\n",
    "img_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feeding it into the untrained model (to check if data is correctly set)\n",
    "test = model(img_batch)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the result out of the output tensor:\n",
    "test.max(1)\n",
    "*_, class_ = test.max(dim=1)\n",
    "class_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change the model so it includes NLLLoss and LogSoftmax\n",
    "model = nn.Sequential(nn.Linear(3072, 512),\n",
    "                      nn.Tanh(),\n",
    "                      nn.Linear(512, 2),\n",
    "                      nn.LogSoftmax(dim=1))\n",
    "\n",
    "loss = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the loss for single example\n",
    "img, label = cifar2[16]\n",
    "img = img.view(-1).unsqueeze(0)\n",
    "loss(model(img), torch.tensor([label]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the training loop, this time using a update-after-each-sample approach\n",
    "import torch.optim as optim\n",
    "def training_loop_sample(n_epochs, model, optimizer, loss, dataset):\n",
    "    \n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        for img, label in dataset:\n",
    "            img = img.view(-1).unsqueeze(0)\n",
    "            loss_s = loss(model(img), torch.tensor([label]))\n",
    "            \n",
    "            loss_s.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "        print(f\"Epoch: {epoch}, Loss: {float(loss_s)}\")\n",
    "            \n",
    "            \n",
    "        \n",
    "n_epochs = 10\n",
    "n_out = 2\n",
    "learning_rate = 1e-2\n",
    "dataset = cifar2[:100]\n",
    "model = nn.Sequential(nn.Linear(3072, 512),\n",
    "                      nn.Tanh(),\n",
    "                      nn.Linear(512, n_out),\n",
    "                      nn.LogSoftmax(dim=1))\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "loss = nn.NLLLoss()\n",
    "\n",
    "training_loop_sample(n_epochs, model, optimizer, loss, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 0.3702535033226013\n",
      "Epoch: 2, Loss: 0.27396026253700256\n",
      "Epoch: 3, Loss: 0.8748315572738647\n",
      "Epoch: 4, Loss: 0.2662021517753601\n",
      "Epoch: 5, Loss: 0.3625795245170593\n",
      "Epoch: 6, Loss: 0.27676188945770264\n",
      "Epoch: 7, Loss: 0.32060787081718445\n",
      "Epoch: 8, Loss: 0.37565717101097107\n",
      "Epoch: 9, Loss: 0.5271724462509155\n",
      "Epoch: 10, Loss: 0.4059954881668091\n"
     ]
    }
   ],
   "source": [
    "#creating a dataloader for our model\n",
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size = 64, shuffle = True)\n",
    "\n",
    "#changing the previous function to work in mini-batches\n",
    "def training_loop_minibatch(n_epochs, model, optimizer, loss, data_loader):\n",
    "    \n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        for imgs, labels in data_loader:\n",
    "            batch_size = imgs.shape[0]\n",
    "            imgs = imgs.view(batch_size,-1)\n",
    "            loss_s = loss(model(imgs), labels)\n",
    "            \n",
    "            loss_s.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "        print(f\"Epoch: {epoch}, Loss: {float(loss_s)}\")\n",
    "            \n",
    "            \n",
    "        \n",
    "n_epochs = 10\n",
    "n_out = 2\n",
    "learning_rate = 1e-2\n",
    "dataset = cifar2[:100]\n",
    "model = nn.Sequential(nn.Linear(3072, 512),\n",
    "                      nn.Tanh(),\n",
    "                      nn.Linear(512, n_out),\n",
    "                      nn.LogSoftmax(dim=1))\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "loss = nn.NLLLoss()\n",
    "data_loader = train_loader\n",
    "\n",
    "training_loop_minibatch(n_epochs, model, optimizer, loss, data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 53.40764331210191\n"
     ]
    }
   ],
   "source": [
    "#checking the accuracy of the model\n",
    "val_loader = torch.utils.data.DataLoader(cifar2_val, batch_size = 64, shuffle = False)\n",
    "correct, total = 0,0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in val_loader:\n",
    "            batch_size = imgs.shape[0]\n",
    "            imgs = imgs.view(batch_size,-1)\n",
    "            prediction = model(imgs)\n",
    "            _, predicted = torch.max(prediction, dim=1)\n",
    "            total += 1\n",
    "            correct += int((predicted == labels).sum())\n",
    "        \n",
    "    print(f\"Accuracy = {correct / total}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
